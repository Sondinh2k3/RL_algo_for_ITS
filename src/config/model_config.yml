# MGMQ Model Configuration
# Các tham số mô hình MGMQ cho training và evaluation

# ============================================
# Network Configuration
# ============================================
network:
  # Name of the traffic network (used as folder name under network/)
  name: grid4x4

  # Base path for network files (relative to project root)
  # If not specified, defaults to: network/{network_name}/
  base_path: null

  # Network files (relative to base_path if base_path is set, or absolute paths)
  net_file: grid4x4.net.xml

  # Route files - can be a single file or comma-separated list
  # Order matters: type definitions should come before trip definitions
  route_files:
    - grid4x4.rou.xml # Vehicle type definitions
    - grid4x4-demo.rou.xml # Demo traffic routes (optional)

  # Detector configuration file (for E2 detector-based rewards)
  detector_file: detector.add.xml

  # Intersection preprocessing config (for phase standardization)
  intersection_config: intersection_config.json

# ============================================
# MGMQ Architecture Parameters
# ============================================
mgmq:
  # GAT (Graph Attention Network) parameters
  gat:
    hidden_dim: 32 # GAT hidden dimension
    output_dim: 32 # GAT output dimension per head
    num_heads: 4 # Number of GAT attention heads

  # GraphSAGE parameters
  graphsage:
    hidden_dim: 64 # GraphSAGE hidden dimension

  # Bi-GRU parameters
  gru:
    hidden_dim: 64 # Bi-GRU hidden dimension

  # Policy and Value network parameters
  policy:
    hidden_dims: [128, 64] # Policy network hidden dimensions

  value:
    hidden_dims: [256, 128, 64] # Value network hidden dimensions (wider to handle 1600-dim input)

  # Regularization
  dropout: 0.1 # Reduced from 0.3 - excessive dropout hurts RL training

  # Temporal observation
  history_length: 1 # Observation history length (window size)

  # Local GNN settings
  local_gnn:
    enabled: false # Use LocalMGMQTorchModel
    max_neighbors: 4 # Maximum neighbors (K) for local GNN
    obs_dim: 48 # 4 features * 12 detectors

# ============================================
# PPO Training Parameters
# ============================================
ppo:
  learning_rate: 0.0003 # Slightly higher LR for faster initial learning, will decay via lr_schedule
  gamma: 0.99 # Discount factor
  lambda_: 0.95 # GAE lambda
  entropy_coeff: 0.01 # Start high, will decay via entropy_coeff_schedule
  # CRITICAL FIX: Schedule points must be in AGENT TIMESTEPS (not iterations)
  # With 16 agents × 67 steps × 2 workers = ~2144 agent_steps/iteration
  # Old schedule [[0,0.01],[100,0.005],[300,0.001]] decayed to min by iteration 1!
  # New schedule: decay over ~100-200 iterations (proper exploration period)
  entropy_coeff_schedule: [[0, 0.01], [200000, 0.005], [500000, 0.001]]
  clip_param: 0.2 # PPO clip parameter
  vf_clip_param: 100.0 # Value function clip - must be large for reward range ~-600 to -700
  vf_loss_coeff: 0.01 # CRITICAL FIX: reduced from 0.5 to prevent VF gradient dominance
  # With vf_loss=60 and policy_loss=0.02:
  #   old (0.5):  VF gradient 1592x larger than policy -> policy can't learn
  #   new (0.01): VF gradient 32x larger -> policy gradient becomes visible
  # VF can still learn via dedicated value network (vf_share_layers=False)
  train_batch_size: 2144 # Fixed: matches 2 workers * 1072 agent_steps/episode
  # With 4 workers: 4 * 1072 = 4288, will auto-collect enough with complete_episodes
  minibatch_size: 268 # SGD minibatch size (2144/268 = 8 minibatches per epoch)
  num_sgd_iter: 10 # Number of SGD iterations
  grad_clip: 10.0 # Increased from 5.0 - GNN with GAT+GraphSAGE+BiGRU needs more headroom
  # Learning rate schedule: linear decay over training
  # Points are [agent_timestep, lr_value]
  lr_schedule: [[0, 0.0003], [500000, 0.0001], [1000000, 0.00003]]

# ============================================
# Training Settings
# ============================================
training:
  num_iterations: 500 # Increased from 200 for proper convergence
  num_workers: 4 # Increased from 2 - more diverse experience per training batch
  num_envs_per_worker: 1 # Environments per worker
  checkpoint_interval: 5 # Checkpoint frequency
  patience: 500 # Early stopping patience (iterations without improvement)
  seed: 42 # Random seed
  use_gpu: false # Use GPU for training
  output_dir: "./results_mgmq" # Output directory

# ============================================
# Reward Configuration
# ============================================
reward:
  # Available reward functions:
  # - diff-waiting-time: Difference in waiting time
  # - queue: Queue length based reward
  # - average-speed: Average speed based reward
  # - pressure: Traffic pressure reward
  # - halt-veh-by-detectors: Halted vehicles by detector
  # - diff-departed-veh: Difference in departed vehicles
  # - teleport-penalty: Penalty for teleported vehicles (severe congestion indicator)
  functions:
    # - halt-veh-by-detectors
    # - diff-departed-veh
    - diff-waiting-time
    - occupancy
    # - average-speed
    # - teleport-penalty   # Uncomment to penalize teleportation (requires time_to_teleport > 0)
  weights: null # Auto-compute equal weights if null

# ============================================
# Environment Configuration
# ============================================
environment:
  num_seconds: 6000 # Simulation duration (synced with grid4x4.sumocfg end=8000)
  max_green: 90 # Maximum green time
  min_green: 5 # Minimum green time
  cycle_time: 90 # Traffic light cycle time
  yellow_time: 3 # Yellow time
  time_to_teleport: 500 # Teleport stuck vehicles (-1 to disable)
  use_phase_standardizer: true # Map 4-value action to signal phases
