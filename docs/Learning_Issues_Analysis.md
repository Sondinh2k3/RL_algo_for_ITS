# ðŸ“Š PhÃ¢n TÃ­ch Váº¥n Äá» Learning KhÃ´ng Cáº£i Thiá»‡n

**NgÃ y phÃ¢n tÃ­ch**: 2025-01-23  
**TÃ¬nh tráº¡ng ban Ä‘áº§u**: episode_reward_mean â‰ˆ -415 (khÃ´ng cáº£i thiá»‡n sau 16 iterations)

---

## ðŸ” Triá»‡u Chá»©ng Quan SÃ¡t ÄÆ°á»£c

| Metric | GiÃ¡ trá»‹ hiá»‡n táº¡i | GiÃ¡ trá»‹ mong Ä‘á»£i |
|--------|-----------------|------------------|
| `episode_reward_mean` | ~-415 (pháº³ng) | Giáº£m dáº§n (cáº£i thiá»‡n) |
| `policy_loss` | 0.005-0.017 | 0.01-0.1 |
| `vf_loss` | 5.0-6.0 | 0.1-1.0 |
| `vf_explained_var` | 0.17-0.55 | >0.7 |
| `entropy` | 5.7-6.2 | Giáº£m dáº§n theo training |
| `policy/vf loss ratio` | 1:500 | ~1:10 |

---

## ðŸ”´ CÃ¡c Váº¥n Äá» NghiÃªm Trá»ng ÄÆ°á»£c PhÃ¡t Hiá»‡n

### 1. **Value Function Scaling Sai** â­ CRITICAL

**Váº¥n Ä‘á»**: Thiáº¿u `vf_loss_coeff` trong PPO config.

**NguyÃªn nhÃ¢n**:
- Máº·c Ä‘á»‹nh `vf_loss_coeff = 1.0` 
- `vf_loss = 5-6` chiáº¿m Æ°u tháº¿ hoÃ n toÃ n trong total loss
- Policy gradients trá»Ÿ nÃªn khÃ´ng Ä‘Ã¡ng ká»ƒ

**Háº­u quáº£**:
- Critic Ä‘Æ°á»£c train quÃ¡ máº¡nh, policy gáº§n nhÆ° khÃ´ng há»c
- `vf_explained_var` tháº¥p (0.17-0.55) cho tháº¥y critic váº«n chÆ°a fit tá»‘t

**Sá»­a chá»¯a**:
```python
# train_mgmq_ppo.py
vf_loss_coeff=0.5  # Giáº£m tá»« 1.0 xuá»‘ng 0.5
```

---

### 2. **Entropy QuÃ¡ Cao - Policy Váº«n Random** â­ HIGH

**Váº¥n Ä‘á»**: `LOG_STD_MAX = 2.0` cho phÃ©p std quÃ¡ lá»›n.

**PhÃ¢n tÃ­ch**:
```
entropy = 0.5 * action_dim * (1 + log(2Ï€) + 2*log_std)
Vá»›i LOG_STD_MAX = 2.0:
  std_max = e^2.0 â‰ˆ 7.39
  entropy_max â‰ˆ 0.5 * 4 * (1 + 1.84 + 4.0) = 13.68
```

Entropy 5.7-6.2 cho tháº¥y policy váº«n quÃ¡ random, khÃ´ng converge.

**Sá»­a chá»¯a**:
```python
# mgmq_model.py
LOG_STD_MAX = 0.5  # Giáº£m tá»« 2.0 xuá»‘ng 0.5
# std_max = e^0.5 â‰ˆ 1.65 (há»£p lÃ½ cho normalized actions)
```

---

### 3. **Reward Function Bug** â­ HIGH

**Váº¥n Ä‘á»**: `_diff_departed_veh_reward()` cÃ³ edge case bug.

**Code cÅ© (BUG)**:
```python
if initial > 0:
    ratio = departed / initial
else:
    if departed > 0:
        ratio = 1.0  # BUG: Cho max reward khi khÃ´ng cÃ³ xe ban Ä‘áº§u!
```

**Váº¥n Ä‘á»**:
- Khi `initial_vehicles = 0` vÃ  `departed > 0` â†’ reward = 3.0 (maximum)
- ÄÃ¢y lÃ  tÃ­n hiá»‡u sai lá»‡ch, khÃ´ng pháº£n Ã¡nh Ä‘Ãºng hiá»‡u quáº£

**Sá»­a chá»¯a**:
```python
MIN_VEHICLES_THRESHOLD = 1.0

if initial >= MIN_VEHICLES_THRESHOLD:
    ratio = departed / initial
else:
    if departed >= MIN_VEHICLES_THRESHOLD:
        ratio = 0.5  # Neutral-positive thay vÃ¬ max
    else:
        return 0.0  # KhÃ´ng cÃ³ xe â†’ neutral
```

---

### 4. **Batch Size QuÃ¡ Nhá»** â­ MEDIUM

**Váº¥n Ä‘á»**: `train_batch_size = 1424` vá»›i 16 agents.

**PhÃ¢n tÃ­ch**:
```
samples_per_agent = 1424 / 16 = 89 samples
â†’ Variance cao trong gradient estimates
```

**Sá»­a chá»¯a**:
```python
train_batch_size=4096,  # TÄƒng tá»« 1424
minibatch_size=128,     # TÄƒng tá»« 64
num_epochs=10,          # TÄƒng tá»« 4
```

---

## âœ… Tá»•ng Há»£p CÃ¡c Thay Äá»•i

### File: `scripts/train_mgmq_ppo.py`

| Parameter | CÅ© | Má»›i | LÃ½ do |
|-----------|-----|-----|-------|
| `vf_loss_coeff` | (máº·c Ä‘á»‹nh 1.0) | 0.5 | CÃ¢n báº±ng policy/vf loss |
| `train_batch_size` | 1424 | 4096 | Giáº£m gradient variance |
| `minibatch_size` | 64 | 128 | Better batch normalization |
| `num_epochs` | 4 | 10 | Thorough updates |

### File: `src/models/mgmq_model.py`

| Parameter | CÅ© | Má»›i | LÃ½ do |
|-----------|-----|-----|-------|
| `LOG_STD_MAX` | 2.0 | 0.5 | Entropy converge nhanh hÆ¡n |

### File: `src/environment/drl_algo/traffic_signal.py`

| Function | Thay Ä‘á»•i |
|----------|----------|
| `_diff_departed_veh_reward()` | Fix edge case khi initial_vehicles â‰ˆ 0 |

---

## ðŸ“ˆ Ká»³ Vá»ng Sau Khi Fix

1. **Policy loss** tÄƒng lÃªn ~0.01-0.05 (cÃ³ gradient Ä‘á»§ lá»›n Ä‘á»ƒ learn)
2. **VF loss** giáº£m dáº§n vá» ~0.5-1.0 khi critic converge
3. **VF explained variance** tÄƒng lÃªn >0.7
4. **Entropy** giáº£m dáº§n khi policy converge
5. **Episode reward** cáº£i thiá»‡n (tÄƒng dáº§n tá»« -415)

---

## ðŸ§ª Khuyáº¿n Nghá»‹ Test

1. **Train Ã­t nháº¥t 100-200 iterations** Ä‘á»ƒ tháº¥y trend
2. **Monitor cÃ¡c metrics sau**:
   - `episode_reward_mean`: Pháº£i cÃ³ xu hÆ°á»›ng tÄƒng
   - `vf_loss`: Pháº£i giáº£m dáº§n
   - `policy_loss`: Pháº£i á»•n Ä‘á»‹nh ~0.01-0.05
   - `entropy`: Pháº£i giáº£m dáº§n
   - `kl_divergence`: Pháº£i < `kl_target` (0.01)

3. **Náº¿u váº«n khÃ´ng improve sau 200 iterations**:
   - Thá»­ dÃ¹ng single reward function: `--reward-fn queue`
   - Giáº£m learning rate xuá»‘ng 1e-4
   - Kiá»ƒm tra observation normalization

---

## ðŸ“š Tham Kháº£o

- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [PPO Implementation Details](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)
- [RLlib PPO Documentation](https://docs.ray.io/en/latest/rllib/algorithms.html#ppo)
